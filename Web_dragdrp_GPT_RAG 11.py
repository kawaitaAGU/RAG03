import streamlit as st
import pandas as pd
from PIL import Image
from openai import OpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from io import BytesIO

# GPT-4o ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
MODEL = "gpt-4o-2024-11-20"

# OpenAI API ã‚­ãƒ¼ã®ç¢ºèª
if "OPENAI_API_KEY" not in st.secrets:
    st.error("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

st.title("æ­¯ç§‘åŒ»å¸«å›½å®¶è©¦é¨“ å•é¡Œè§£æï¼†é¡é¡Œç”Ÿæˆ")

# sample.csvã®èª­ã¿è¾¼ã¿ï¼ˆäº‹å‰ã«ç”¨æ„ã•ã‚ŒãŸRAGãƒ‡ãƒ¼ã‚¿ï¼‰
df = pd.read_csv("sample.csv")
questions_list = df["question"].astype(str).tolist()

uploaded_file = st.file_uploader("è©¦é¨“å•é¡Œç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["png", "jpg", "jpeg"])

if uploaded_file:
    image = Image.open(uploaded_file)
    st.image(image, caption="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå•é¡Œç”»åƒ", use_column_width=True)

    # Step 1: Vision GPTã§ç”»åƒã‹ã‚‰å•é¡Œæ–‡ã¨é¸æŠè‚¢ã‚’æŠ½å‡º
    with st.spinner("ç”»åƒã‹ã‚‰å•é¡Œæ–‡ã¨é¸æŠè‚¢ã‚’æŠ½å‡ºä¸­..."):
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯è©¦é¨“å•é¡Œã®OCRã¨æ§‹é€ åŒ–ã‚’è¡Œã†AIã§ã™ã€‚"},
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "ä»¥ä¸‹ã¯æ­¯ç§‘åŒ»å¸«å›½å®¶è©¦é¨“ã®å•é¡Œç”»åƒã§ã™ã€‚å•é¡Œæ–‡ã¨é¸æŠè‚¢ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§æŠ½å‡ºã—ã€ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\nã€å•é¡Œæ–‡ã€‘\n...\nã€é¸æŠè‚¢ã€‘\na. ...\nb. ...\nc. ...\nd. ...\ne. ..."
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": "data:image/png;base64," + base64.b64encode(uploaded_file.read()).decode()
                            },
                        },
                    ],
                },
            ],
            temperature=0.2,
        )
        question_text = response.choices[0].message.content
        st.markdown("### ğŸ” æŠ½å‡ºã•ã‚ŒãŸå•é¡Œæ–‡ã¨é¸æŠè‚¢")
        st.markdown(question_text)

    # Step 2: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«ã‚ˆã‚‹RAGæ¤œç´¢
    with st.spinner("é¡ä¼¼å•é¡Œã‚’æ¤œç´¢ä¸­..."):
        vectorizer = TfidfVectorizer()
        vectors = vectorizer.fit_transform([question_text] + questions_list)
        cos_sim = cosine_similarity(vectors[0:1], vectors[1:]).flatten()
        top_k = cos_sim.argsort()[-3:][::-1]
        similar_entries = df.iloc[top_k][["question", "answer", "explanation"]].to_dict(orient="records")

    # Step 3: GPTã«è§£ç­”ãƒ»è§£èª¬ãƒ»é¡é¡Œä½œæˆã‚’ä¾é ¼
    with st.spinner("GPTã«ã‚ˆã‚‹è§£æã¨å‡ºåŠ›ã‚’ç”Ÿæˆä¸­..."):
        prompt = f"""
ä»¥ä¸‹ã¯å›½å®¶è©¦é¨“å•é¡Œã§ã™ã€‚ã¾ãšæ­£è§£ã‚’é¸ã³ã€é¸æŠè‚¢ã”ã¨ã«ç†ç”±ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚
ãã®å¾Œã€é¡ä¼¼å•é¡Œã‚’3å•ä½œæˆã—ã€ãã‚Œãã‚Œæ­£è§£ã¨è§£èª¬ã‚’åŠ ãˆã¦ãã ã•ã„ã€‚

ã€å•é¡Œã€‘
{question_text}

ã€å‚è€ƒãƒ‡ãƒ¼ã‚¿ï¼ˆéå»å•RAGã‚ˆã‚Šï¼‰ã€‘
{similar_entries}
"""

        completion = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªæ­¯ç§‘åŒ»å¸«å›½å®¶è©¦é¨“ã®è§£èª¬è€…ã§ã™ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
        )
        st.markdown("### âœ… GPTã«ã‚ˆã‚‹è§£ç­”ã¨è§£èª¬")
        st.markdown(completion.choices[0].message.content)