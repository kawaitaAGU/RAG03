import streamlit as st
from PIL import Image
import pandas as pd
from io import BytesIO
from openai import OpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# OpenAI API Key èª­ã¿è¾¼ã¿
if "OPENAI_API_KEY" not in st.secrets:
    st.error("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlitã®Secretsã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# CSVãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv("sample.csv")
if "å•é¡Œæ–‡" not in df.columns or "a" not in df.columns or "è§£ç­”" not in df.columns:
    st.error("CSVã«å¿…è¦ãªåˆ—ï¼ˆå•é¡Œæ–‡, a, è§£ç­”ï¼‰ãŒã‚ã‚Šã¾ã›ã‚“")
    st.stop()

# é¡ä¼¼å•é¡Œæ¤œç´¢é–¢æ•°
def retrieve_similar_questions(query, top_k=3):
    vectorizer = TfidfVectorizer().fit(df["å•é¡Œæ–‡"])
    tfidf_matrix = vectorizer.transform(df["å•é¡Œæ–‡"])
    query_vec = vectorizer.transform([query])
    scores = cosine_similarity(query_vec, tfidf_matrix)[0]
    top_indices = scores.argsort()[-top_k:][::-1]
    return df.iloc[top_indices]

# Streamlit UI
st.title("ç”»åƒãƒ™ãƒ¼ã‚¹æ­¯ç§‘å›½å®¶è©¦é¨“å•é¡Œè§£æï¼‹RAGã‚·ã‚¹ãƒ†ãƒ ")
uploaded_file = st.file_uploader("å›½å®¶è©¦é¨“å•é¡Œã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["png", "jpg", "jpeg"])

if uploaded_file:
    st.image(uploaded_file, caption="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ", use_column_width=True)

    # ç”»åƒã‚’GPT-4oã«é€ä¿¡ã—å•é¡Œè§£æ
    image_bytes = uploaded_file.read()
    image_analysis = client.chat.completions.create(
        model="gpt-4o-2024-11-20",
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯å›½å®¶è©¦é¨“å•é¡Œã«ç‰¹åŒ–ã—ãŸOCRã¨å•é¡Œè§£æã‚’è¡Œã†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "ã“ã®ç”»åƒã«å«ã¾ã‚Œã‚‹å›½å®¶è©¦é¨“å•é¡Œã®å•é¡Œæ–‡ã€é¸æŠè‚¢ï¼ˆaã€œeï¼‰ã€æ­£è§£ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚"},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_bytes.hex()}", "detail": "high"}}
                ],
            },
        ],
        temperature=0.3,
        max_tokens=2000
    )

    parsed_text = image_analysis.choices[0].message.content
    st.markdown("### ğŸ§¾ è§£æçµæœ")
    st.markdown(parsed_text)

    # é¡ä¼¼å•é¡Œã‚’RAGæ¤œç´¢
    similar_df = retrieve_similar_questions(parsed_text, top_k=3)

    st.markdown("### ğŸ” é¡ä¼¼å•é¡Œ (RAGçµæœ)")
    for i, row in similar_df.iterrows():
        st.markdown(f"**å•é¡Œ**: {row['å•é¡Œæ–‡']}")
        st.markdown(f"a: {row['a']}ã€€b: {row['b']}ã€€c: {row['c']}ã€€d: {row['d']}ã€€e: {row['e']}")
        st.markdown(f"**æ­£è§£**: {row['è§£ç­”']}")
        st.markdown("---")

    # GPTã«å†é€ä¿¡ã—ã¦è§£ç­”ï¼†é¡é¡Œç”Ÿæˆ
    rag_context = "\n\n".join(
        f"å•é¡Œ: {row['å•é¡Œæ–‡']}\na: {row['a']}\nb: {row['b']}\nc: {row['c']}\nd: {row['d']}\ne: {row['e']}\næ­£è§£: {row['è§£ç­”']}"
        for _, row in similar_df.iterrows()
    )

    final_response = client.chat.completions.create(
        model="gpt-4o-2024-11-20",
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯å›½å®¶è©¦é¨“å•é¡Œã®å‡ºé¡Œæ„å›³ã‚’æ·±ãç†è§£ã—ã€è§£èª¬ã¨é¡é¡Œã‚’ç”Ÿæˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
            {"role": "user", "content": f"""
ä»¥ä¸‹ã¯è§£æã•ã‚ŒãŸæœªçŸ¥ã®å•é¡Œã§ã™ã€‚

{parsed_text}

ä»¥ä¸‹ã¯RAGï¼ˆéå»å•é¡Œã®é¡ä¼¼å•é¡Œï¼‰ã§ã™ï¼š

{rag_context}

ã“ã®æƒ…å ±ã‚’ã‚‚ã¨ã«ã€1. å•é¡Œæ–‡ã¨å„é¸æŠè‚¢ã®è§£èª¬ã€2. é¡ä¼¼å•é¡Œã‚’3å•ã€ãã‚Œãã‚Œã®é¸æŠè‚¢ã”ã¨ã®èª¬æ˜ã¨æ­£è§£ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
"""}
        ],
        temperature=0.3,
        max_tokens=3000
    )

    st.markdown("### âœ… è§£ç­”ãƒ»è§£èª¬ã¨é¡é¡Œ")
    st.markdown(final_response.choices[0].message.content)
